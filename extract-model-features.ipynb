{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template I use to extract features from vision models. Features are saved as a .pkl dictionary where keys are the layer names (string) and values are the feature matrices (numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "print(\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This line is for the HPC. \n",
    "# os.environ['TORCH_HOME'] = '/home/mualla/.cache/torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_activations(model, layer_names, image_tensor):\n",
    "    \"\"\"\n",
    "    Get the activations of specified layers in response to input data, handling large batches by\n",
    "    splitting them into smaller batches of size 100, and concatenating the results. The activations\n",
    "    are detached from the computation graph and moved to the CPU before storage.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to probe.\n",
    "        layer_names (list): List of names of the layers to probe.\n",
    "        image_tensor (torch.Tensor): Batch of images to feed through the model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are layer names and values are concatenated activations for all batches,\n",
    "              with each tensor detached and moved to CPU.\n",
    "    \"\"\"\n",
    "    # Ensure layer_names is a list\n",
    "    if not isinstance(layer_names, list):\n",
    "        layer_names = [layer_names]\n",
    "\n",
    "    activations = {name: [] for name in layer_names}\n",
    "    hooks = []\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # Detach activations from computation graph and move to CPU\n",
    "            activations[name].append(output.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for each specified layer\n",
    "    for name in layer_names:\n",
    "        try:\n",
    "            layer = dict([*model.named_modules()])[name]\n",
    "            print(f\"Layer {layer} is registered hook.\")\n",
    "            \n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Layer {name} not found in model.\")\n",
    "            \n",
    "        hook = layer.register_forward_hook(get_activation(name))\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Handle large batch sizes by processing smaller batches of size 100\n",
    "    batch_size = 100\n",
    "    for i in range(0, image_tensor.size(0), batch_size):\n",
    "        batch = image_tensor[i:i + batch_size]\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            model(batch)\n",
    "        \n",
    "    # Concatenate the activations for each layer across all batches\n",
    "    for name in activations:\n",
    "        activations[name] = torch.cat(activations[name], dim=0).numpy()\n",
    "        # activations[name] = torch.cat(activations[name], dim=0)\n",
    "\n",
    "    # Remove hooks after completion\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dictionary, it maps the model names to their corresponding loading function and layers we want to extract activations from\n",
    "model_dict = {\n",
    "    'alexnet': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True),\n",
    "        'layers': [\"features.0\", \"features.2\", \"features.3\", \"features.5\", \"features.6\", \"features.8\", \"features.12\", \"classifier.1\", \"classifier.4\"]\n",
    "    },\n",
    "    'vgg-16': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True),\n",
    "        'layers': [\"features.4\", \"features.9\", \"features.16\", \"features.23\", \"features.30\", \"classifier.0\", \"classifier.3\"]\n",
    "    },\n",
    "    'resnet-50': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True),\n",
    "        'layers': [\"layer1.0.bn1\",\"layer1.1.bn1\",\"layer1.2.bn1\",\"layer2.0.bn1\",\"layer2.1.bn1\",\"layer2.2.bn1\",\"layer3.1.bn1\",\"layer3.2.bn1\"]\n",
    "    },\n",
    "    'resnet-101': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True),\n",
    "        'layers': [\"layer3.2.bn1\"]\n",
    "    },\n",
    "    'resnet-152': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True),\n",
    "        'layers': [\"layer3.3.bn1\"]\n",
    "    },\n",
    "    \n",
    "    'densenet-121': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True),\n",
    "        'layers': []\n",
    "    },\n",
    "    \n",
    "    'densenet-201': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True),\n",
    "        'layers': []\n",
    "    },\n",
    "\n",
    "    'densenet-169': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True),\n",
    "        'layers': []\n",
    "    },\n",
    "    \n",
    "    'squeezenet-1_0': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True),\n",
    "        'layers': [\"features.4.expand3x3\",\"features.3.expand3x3\",\"features.5.expand3x3\",\"features.7.expand3x3\",\"features.8.expand3x3\",\"features.9.expand3x3\",\"features.10.expand3x3\",\"features.12.expand3x3\"]\n",
    "    },\n",
    "    'squeezenet-1_1': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True),\n",
    "        'layers': [\"features.3.expand3x3_activation\",\"features.4.expand3x3_activation\",\"features.6.expand3x3_activation\",\"features.7.expand3x3_activation\",\"features.9.expand3x3_activation\"]\n",
    "    },\n",
    "    'inception-v3': {\n",
    "        'load': lambda: torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True),\n",
    "        'layers': [\"Mixed_7a.branch3x3_1.bn\"]\n",
    "    },\n",
    "    'resnext-wsl': {\n",
    "        'load': lambda: torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl'),\n",
    "        'layers': [\"layer3.3.relu\"]\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# Define the folder path where the images are stored\n",
    "images_folder_path = './data/images'\n",
    "\n",
    "# Choose the models you want to extract from\n",
    "models = ['resnet-50',\n",
    "          'alexnet',\n",
    "          'vgg-16' \n",
    "          'resnet-50',\n",
    "          'resnet-101', \n",
    "          'resnet-152', \n",
    "          'densenet-121', \n",
    "          'densenet-201', \n",
    "          'densenet-169', \n",
    "          'squeezenet-1_0',\n",
    "          'squeezenet-1_1',\n",
    "          'inception-v3',\n",
    "          'resnext-wsl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the models\n",
    "for current_model in models: \n",
    "    \n",
    "    model = model_dict[current_model]['load']()\n",
    "    layers = model_dict[current_model]['layers']\n",
    "    \n",
    "    print(f\"Loaded the model {current_model}\")\n",
    "    # Print the layer names (in case I want to see and update the layers)     \n",
    "    for name, parameter in model.named_parameters():\n",
    "        print(name)\n",
    "       \n",
    "    # Move the model to device and freeze the layers. \n",
    "    model.eval().to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Initialize the tensors list. \n",
    "    img_tensors_list = []\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Iterate through the images and transform before passing to the model. \n",
    "    for i in range(1600):\n",
    "            filename = f\"im{i}.png\"\n",
    "            image_path = os.path.join(images_folder_path, filename)\n",
    "            \n",
    "            if os.path.exists(image_path):\n",
    "                img = Image.open(image_path).convert(\"RGB\")\n",
    "                input_tensor = preprocess(img)\n",
    "                input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "                img_tensors_list.append(input_batch)\n",
    "            else:\n",
    "                print(f\"Warning: File {filename} not found. Skipping.\")\n",
    "                \n",
    "    # Put the transformed images together and pass them to the model to get the activations.        \n",
    "    big_tensor = torch.cat(img_tensors_list, dim=0)      \n",
    "    activations = get_layer_activations(model, layers, big_tensor)\n",
    "    \n",
    "    # Save activations as pkl. \n",
    "    with open(f'./results/model_features/{current_model}_multiple_layers_features.pkl', 'wb') as f:\n",
    "        pickle.dump(activations, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
